{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9c51fd",
   "metadata": {},
   "source": [
    "# Full Training Workflow (Tiny Pretraining Demonstration)\n",
    "Train a tiny GPT-2 model config on a small corpus just to illustrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "# Use an off-the-shelf tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# Small text dataset for LM (wikitext-2-raw)\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "def tokenize(ex):\n",
    "    return tok(ex[\"text\"])\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Group into fixed-size blocks for efficient LM training\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concat = sum(examples[\"input_ids\"], [])\n",
    "    total_len = (len(concat) // block_size) * block_size\n",
    "    result = {\"input_ids\": [concat[i:i+block_size] for i in range(0, total_len, block_size)]}\n",
    "    result[\"attention_mask\"] = [[1]*block_size]*len(result[\"input_ids\"])\n",
    "    return result\n",
    "\n",
    "lm_ds = tokenized.map(group_texts, batched=True)\n",
    "lm_ds = lm_ds[\"train\"].select(range(5000)).train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tok),\n",
    "    n_layer=4, n_head=4, n_embd=256,\n",
    "    n_positions=block_size, n_ctx=block_size\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"tiny-gpt2-pretrain\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=100,\n",
    "    save_steps=400,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=lm_ds[\"train\"], eval_dataset=lm_ds[\"test\"], data_collator=collator)\n",
    "trainer.train()\n",
    "\n",
    "eval_res = trainer.evaluate()\n",
    "perplexity = math.exp(eval_res[\"eval_loss\"])\n",
    "print(\"Perplexity:\", perplexity)\n",
    "model.save_pretrained(\"tiny-gpt2-pretrain/model\")\n",
    "tok.save_pretrained(\"tiny-gpt2-pretrain/tokenizer\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
