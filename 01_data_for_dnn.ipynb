{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75894ac",
   "metadata": {},
   "source": [
    "# Data for Training Deep Neural Networks\n",
    "Sourcing, cleaning, tokenization, mapping, and dataset cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U datasets transformers pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example: IMDB for text classification\n",
    "ds = load_dataset(\"imdb\")\n",
    "print(ds)\n",
    "\n",
    "# Train/val/test split (IMDB already split; we create a small validation here)\n",
    "small_train = ds[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "small_test  = ds[\"test\"].shuffle(seed=42).select(range(2000))\n",
    "raw = DatasetDict({\"train\": small_train, \"test\": small_test})\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning/tokenization\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess(ex):\n",
    "    return tok(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tok_ds = raw.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "tok_ds = tok_ds.rename_column(\"label\", \"labels\")\n",
    "tok_ds.set_format(type=\"torch\")\n",
    "tok_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data card (example template)\n",
    "data_card = {\n",
    "    \"source\": \"IMDB from Hugging Face Datasets\",\n",
    "    \"license\": \"See dataset card\",\n",
    "    \"splits\": {k: len(v) for k, v in tok_ds.items()},\n",
    "    \"processing\": \"Lowercasing via tokenizer; truncation to 256 tokens; no deduping for demo\",\n",
    "    \"known_issues\": [\"Small subset; sentiment domain only; minimal cleaning\"],\n",
    "}\n",
    "import json, pprint\n",
    "pprint.pp(data_card)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef0070",
   "metadata": {},
   "source": [
    "**Next**: Use these prepared datasets in SFT/PEFT notebooks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
