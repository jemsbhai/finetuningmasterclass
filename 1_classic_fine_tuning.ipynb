{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classic Fine-tuning Example\n",
        "\n",
        "This notebook demonstrates full fine-tuning of a pre-trained language model for text classification.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU Runtime (T4 or better)\n",
        "- ~8GB VRAM for this example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load IMDB dataset for sentiment analysis\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Use a smaller subset for quick training\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nExample: {train_dataset[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Pre-trained Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use a small BERT model for this example\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with classification head\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,  # Binary classification\n",
        "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
        "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(f\"Total training steps: {len(tokenized_train) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load accuracy metric\n",
        "accuracy = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Trainer and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"./fine_tuned_model\")\n",
        "\n",
        "# Print training results\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate on test set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"Evaluation Results:\")\n",
        "print(f\"Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Model with Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "    \n",
        "    label = model.config.id2label[predicted_class]\n",
        "    confidence = predictions[0][predicted_class].item()\n",
        "    \n",
        "    return label, confidence\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"Terrible film. Waste of time and money.\",\n",
        "    \"The plot was interesting but the execution could have been better.\",\n",
        "    \"Outstanding performance by the lead actor. Highly recommend!\",\n",
        "    \"I fell asleep halfway through. So boring.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    label, confidence = predict_sentiment(text)\n",
        "    print(f\"Text: {text[:50]}...\")\n",
        "    print(f\"Prediction: {label} (Confidence: {confidence:.2%})\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Full fine-tuning** updates all model parameters\n",
        "2. **Memory requirements** scale with model size\n",
        "3. **Training time** depends on dataset size and epochs\n",
        "4. **Mixed precision** (fp16) reduces memory usage\n",
        "5. **Evaluation metrics** guide model selection\n",
        "\n",
        "### Next Steps\n",
        "- Try different learning rates\n",
        "- Experiment with larger models\n",
        "- Use more training data\n",
        "- Implement early stopping\n",
        "- Try different optimizers (AdamW, Adafactor)"
      ]
    }
  ]
}
