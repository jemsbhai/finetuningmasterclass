{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-tuning Example\n",
        "\n",
        "This notebook demonstrates Low-Rank Adaptation (LoRA) for efficient fine-tuning of large language models.\n",
        "\n",
        "**Key Benefits:**\n",
        "- 0.1-1% trainable parameters\n",
        "- 10x less memory usage\n",
        "- Multiple task adapters on single model\n",
        "- No inference latency after merging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use a smaller model for demonstration\n",
        "model_name = \"microsoft/phi-2\"  # 2.7B parameters\n",
        "# Alternative: \"meta-llama/Llama-2-7b-hf\" (requires HF token)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model in 8-bit for memory efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,  # Quantization for memory efficiency\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank - higher = more capacity but more parameters\n",
        "    lora_alpha=32,  # Scaling parameter\n",
        "    target_modules=[\n",
        "        \"q_proj\",  # Query projection\n",
        "        \"v_proj\",  # Value projection\n",
        "        # \"k_proj\",  # Optional: Key projection\n",
        "        # \"o_proj\",  # Optional: Output projection\n",
        "    ],\n",
        "    lora_dropout=0.1,  # Dropout for regularization\n",
        "    bias=\"none\",  # Don't train biases\n",
        "    task_type=TaskType.CAUSAL_LM,  # Task type\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load a small instruction dataset\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "\n",
        "# Use subset for quick training\n",
        "dataset = dataset.select(range(1000))\n",
        "\n",
        "# Format prompts\n",
        "def format_instruction(sample):\n",
        "    instruction = f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Input:\n",
        "{sample['input'] if sample['input'] else 'N/A'}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "    return instruction\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    instructions = [format_instruction(sample) for sample in examples]\n",
        "    model_inputs = tokenizer(\n",
        "        instructions,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda x: tokenize_function([x]),\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(tokenized_dataset)}\")\n",
        "print(f\"\\nExample instruction:\\n{format_instruction(dataset[0])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
        "    warmup_steps=50,\n",
        "    max_steps=200,  # Limit steps for demo\n",
        "    learning_rate=3e-4,  # Higher LR for LoRA\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    evaluation_strategy=\"no\",  # No eval for demo\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "print(f\"Training configuration ready\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Total steps: {training_args.max_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.save_pretrained(\"./lora_adapter\")\n",
        "print(\"\\nLoRA adapter saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference with LoRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_response(instruction, input_text=\"\"):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text if input_text else 'N/A'}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the response part\n",
        "    response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test examples\n",
        "test_instructions = [\n",
        "    (\"What is machine learning?\", \"\"),\n",
        "    (\"Translate the following to French:\", \"Hello, how are you?\"),\n",
        "    (\"Summarize this text:\", \"LoRA is a technique for efficient fine-tuning of large language models. It works by decomposing weight updates into low-rank matrices.\"),\n",
        "    (\"Write a haiku about AI\", \"\"),\n",
        "]\n",
        "\n",
        "for instruction, input_text in test_instructions:\n",
        "    print(f\"\\nInstruction: {instruction}\")\n",
        "    if input_text:\n",
        "        print(f\"Input: {input_text}\")\n",
        "    response = generate_response(instruction, input_text)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. QLoRA - 4-bit Training (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# QLoRA configuration for 4-bit training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True\n",
        "# )\n",
        "\n",
        "print(\"QLoRA allows training 70B+ models on consumer GPUs!\")\n",
        "print(\"- 4-bit base model\")\n",
        "print(\"- 16-bit LoRA adapters\")\n",
        "print(\"- Matches full fine-tuning performance\")\n",
        "print(\"- 10x memory reduction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Merge and Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Merge LoRA weights with base model\n",
        "# This creates a standard model without LoRA overhead\n",
        "\n",
        "# merged_model = model.merge_and_unload()\n",
        "# merged_model.save_pretrained(\"./merged_model\")\n",
        "\n",
        "print(\"After merging:\")\n",
        "print(\"- No additional inference latency\")\n",
        "print(\"- Standard model format\")\n",
        "print(\"- Can be deployed normally\")\n",
        "print(\"\\nAdapter size: ~100MB vs Full model: ~5-10GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### LoRA Benefits\n",
        "1. **Efficiency**: Only 0.1-1% of parameters trainable\n",
        "2. **Memory**: 10x reduction in GPU memory\n",
        "3. **Speed**: 3-10x faster training\n",
        "4. **Flexibility**: Multiple task adapters on one model\n",
        "5. **Deployment**: No inference overhead after merging\n",
        "\n",
        "### Best Practices\n",
        "- Start with r=8-16 for rank\n",
        "- Set alpha = 2 × rank\n",
        "- Target Q and V projections first\n",
        "- Use higher learning rates (1e-4 to 1e-3)\n",
        "- Apply dropout (0.05-0.1) to prevent overfitting\n",
        "\n",
        "### When to Use LoRA\n",
        "- Limited GPU memory\n",
        "- Need multiple task-specific models\n",
        "- Rapid experimentation\n",
        "- Edge deployment with adapter swapping"
      ]
    }
  ]
}
