{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aa4d64",
   "metadata": {},
   "source": [
    "# Postâ€‘Training for Reasoning with DPO (Direct Preference Optimization)\n",
    "Train on synthetic preference pairs (chosen vs rejected) to steer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dade9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers accelerate datasets trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "base = \"distilgpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(base)\n",
    "tok.pad_token = tok.eos_token\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(base)  # frozen reference\n",
    "policy = AutoModelForCausalLM.from_pretrained(base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90399a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic preference data (math reasoning toy)\n",
    "pairs = []\n",
    "problems = [\n",
    "    \"Add 27 and 35.\",\n",
    "    \"What is 9 times 7?\",\n",
    "    \"If you have 12 apples and eat 5, how many remain?\"\n",
    "]\n",
    "for p in problems*50:\n",
    "    chosen = f\"Let's think step by step. {p} The answer is: \"  # 'better' style (with reasoning)\n",
    "    rejected = f\"{p} Answer: \"                                # terse style\n",
    "    pairs.append({\"prompt\": p, \"chosen\": chosen+\"(correct)\", \"rejected\": rejected+\"(correct)\"})\n",
    "ds = Dataset.from_list(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DPOConfig(\n",
    "    output_dir=\"dpo-reasoning-demo\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    beta=0.1,   # inverse temperature; tune per task\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "trainer = DPOTrainer(\n",
    "    model=policy,\n",
    "    ref_model=ref_model,\n",
    "    args=cfg,\n",
    "    beta=cfg.beta,\n",
    "    train_dataset=ds,\n",
    "    tokenizer=tok,\n",
    "    max_length=256\n",
    ")\n",
    "trainer.train()\n",
    "policy.save_pretrained(\"dpo-reasoning-demo/model\")\n",
    "tok.save_pretrained(\"dpo-reasoning-demo/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de302703",
   "metadata": {},
   "source": [
    "> In practice: curate real preference data, control KL/divergence from the SFT policy, and evaluate with blinded pairwise tests."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
