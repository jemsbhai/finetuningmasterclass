{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RLHF with DPO (Direct Preference Optimization)\n",
        "\n",
        "This notebook demonstrates Direct Preference Optimization (DPO), a simpler alternative to PPO for RLHF.\n",
        "\n",
        "**DPO Advantages:**\n",
        "- No separate reward model needed\n",
        "- More stable training than PPO\n",
        "- Direct learning from preferences\n",
        "- Simpler implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft trl bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer\n",
        "from datasets import load_dataset, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding DPO\n",
        "\n",
        "DPO directly optimizes the policy to match human preferences without a separate reward model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"DPO Training Pipeline:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n1. Start with SFT model (instruction-tuned)\")\n",
        "print(\"2. Collect preference data:\")\n",
        "print(\"   - Prompt\")\n",
        "print(\"   - Chosen response (preferred)\")\n",
        "print(\"   - Rejected response (not preferred)\")\n",
        "print(\"3. Train with DPO loss:\")\n",
        "print(\"   - Increase likelihood of chosen\")\n",
        "print(\"   - Decrease likelihood of rejected\")\n",
        "print(\"   - Maintain KL divergence with reference\")\n",
        "print(\"\\nNo reward model needed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Preference Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a small preference dataset for demonstration\n",
        "# In practice, you would use human-labeled preferences\n",
        "\n",
        "preference_data = [\n",
        "    {\n",
        "        \"prompt\": \"What is the capital of France?\",\n",
        "        \"chosen\": \"The capital of France is Paris, a beautiful city known for the Eiffel Tower, rich history, and cultural significance.\",\n",
        "        \"rejected\": \"paris\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain machine learning in simple terms.\",\n",
        "        \"chosen\": \"Machine learning is a type of artificial intelligence where computers learn patterns from data to make predictions or decisions, without being explicitly programmed for every scenario. Think of it like teaching a computer to recognize patterns the way humans do.\",\n",
        "        \"rejected\": \"ML is when computers learn stuff from data using algorithms and math to do things.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"How do I stay healthy?\",\n",
        "        \"chosen\": \"To maintain good health: 1) Eat a balanced diet with fruits, vegetables, and whole grains, 2) Exercise regularly (at least 30 minutes daily), 3) Get 7-9 hours of sleep, 4) Stay hydrated, 5) Manage stress through relaxation techniques, 6) Regular check-ups with healthcare providers.\",\n",
        "        \"rejected\": \"just eat good food and exercise sometimes\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Write a professional email opening.\",\n",
        "        \"chosen\": \"Dear [Name],\\n\\nI hope this email finds you well. I am writing to discuss...\",\n",
        "        \"rejected\": \"Hey there, wanted to talk about...\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What is climate change?\",\n",
        "        \"chosen\": \"Climate change refers to long-term shifts in global temperatures and weather patterns. While natural variations occur, scientific evidence shows human activities, particularly burning fossil fuels, have been the dominant driver since the mid-20th century, leading to global warming and environmental impacts.\",\n",
        "        \"rejected\": \"weather getting hotter because of pollution and stuff\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Duplicate data for larger training set\n",
        "preference_data = preference_data * 20  # 100 examples\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = Dataset.from_list(preference_data)\n",
        "eval_dataset = Dataset.from_list(preference_data[:10])  # Small eval set\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "print(f\"\\nExample preference:\")\n",
        "print(f\"Prompt: {train_dataset[0]['prompt']}\")\n",
        "print(f\"Chosen: {train_dataset[0]['chosen'][:50]}...\")\n",
        "print(f\"Rejected: {train_dataset[0]['rejected']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = \"microsoft/phi-2\"  # Small model for demo\n",
        "\n",
        "# Quantization config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model and reference model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model_ref = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Important for DPO\n",
        "\n",
        "print(f\"Models loaded: {model_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure LoRA for Efficient Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # Low rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"LoRA configured for efficient DPO training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure DPO Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./dpo_results\",\n",
        "    num_train_epochs=1,  # Quick demo\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",  # Don't save for demo\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    warmup_steps=10,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# DPO specific arguments\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    model_ref,\n",
        "    args=training_args,\n",
        "    beta=0.1,  # KL penalty coefficient\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    max_prompt_length=128,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "print(\"DPO trainer configured\")\n",
        "print(f\"Beta (KL coefficient): 0.1\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train with DPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train\n",
        "print(\"Starting DPO training...\")\n",
        "print(\"This trains the model to prefer 'chosen' over 'rejected' responses\")\n",
        "print()\n",
        "\n",
        "dpo_trainer.train()\n",
        "\n",
        "print(\"\\nDPO training completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test the Aligned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_response(prompt, model):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How do I stay healthy?\",\n",
        "    \"Explain machine learning in simple terms.\",\n",
        "    \"Write a professional email opening.\",\n",
        "]\n",
        "\n",
        "print(\"Testing DPO-aligned model:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    response = generate_response(prompt, model)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Understanding DPO Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"DPO Loss Function:\")\n",
        "print(\"=\"*50)\n",
        "print()\n",
        "print(\"L_DPO = -log σ(β * log[π(y_chosen|x)/π_ref(y_chosen|x)]\")\n",
        "print(\"              - β * log[π(y_rejected|x)/π_ref(y_rejected|x)])\")\n",
        "print()\n",
        "print(\"Where:\")\n",
        "print(\"- π: Policy (model being trained)\")\n",
        "print(\"- π_ref: Reference policy (frozen)\")\n",
        "print(\"- y_chosen: Preferred response\")\n",
        "print(\"- y_rejected: Non-preferred response\")\n",
        "print(\"- β: KL penalty coefficient\")\n",
        "print(\"- σ: Sigmoid function\")\n",
        "print()\n",
        "print(\"This loss:\")\n",
        "print(\"1. Increases likelihood of chosen responses\")\n",
        "print(\"2. Decreases likelihood of rejected responses\")\n",
        "print(\"3. Prevents model from deviating too far from reference\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. PPO vs DPO Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison_data = {\n",
        "    \"Aspect\": [\n",
        "        \"Reward Model\",\n",
        "        \"Training Complexity\",\n",
        "        \"Memory Usage\",\n",
        "        \"Training Stability\",\n",
        "        \"Implementation\",\n",
        "        \"Hyperparameters\",\n",
        "        \"Performance\"\n",
        "    ],\n",
        "    \"PPO\": [\n",
        "        \"Required\",\n",
        "        \"High\",\n",
        "        \"High (4 models)\",\n",
        "        \"Can be unstable\",\n",
        "        \"Complex\",\n",
        "        \"Many to tune\",\n",
        "        \"95-100%\"\n",
        "    ],\n",
        "    \"DPO\": [\n",
        "        \"Not needed\",\n",
        "        \"Low\",\n",
        "        \"Low (2 models)\",\n",
        "        \"Very stable\",\n",
        "        \"Simple\",\n",
        "        \"Few (mainly β)\",\n",
        "        \"90-95%\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nPPO vs DPO Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Recommendation: Start with DPO for simplicity,\")\n",
        "print(\"use PPO only if you need maximum performance.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices for DPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"DPO Best Practices:\")\n",
        "print(\"=\"*50)\n",
        "print()\n",
        "print(\"1. Data Quality:\")\n",
        "print(\"   - Ensure clear preference distinctions\")\n",
        "print(\"   - Balance different types of preferences\")\n",
        "print(\"   - Include diverse prompts\")\n",
        "print()\n",
        "print(\"2. Hyperparameters:\")\n",
        "print(\"   - β: Start with 0.1-0.2\")\n",
        "print(\"   - Learning rate: 1e-6 to 5e-5\")\n",
        "print(\"   - Batch size: As large as memory allows\")\n",
        "print()\n",
        "print(\"3. Model Selection:\")\n",
        "print(\"   - Start with good SFT model\")\n",
        "print(\"   - Ensure reference model is fixed\")\n",
        "print(\"   - Consider LoRA for efficiency\")\n",
        "print()\n",
        "print(\"4. Evaluation:\")\n",
        "print(\"   - Monitor chosen vs rejected rewards\")\n",
        "print(\"   - Check for reward hacking\")\n",
        "print(\"   - Human evaluation for quality\")\n",
        "print()\n",
        "print(\"5. Common Issues:\")\n",
        "print(\"   - If model degenerates: Increase β\")\n",
        "print(\"   - If no improvement: Decrease β\")\n",
        "print(\"   - If unstable: Reduce learning rate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### DPO Advantages\n",
        "1. **Simpler than PPO**: No reward model or value network\n",
        "2. **Stable training**: Direct optimization without RL loops\n",
        "3. **Memory efficient**: Only needs model + reference\n",
        "4. **Easy to implement**: Standard supervised learning setup\n",
        "\n",
        "### When to Use DPO\n",
        "- Starting RLHF experiments\n",
        "- Limited computational resources\n",
        "- Need stable, predictable training\n",
        "- Preference data available\n",
        "\n",
        "### When to Use PPO Instead\n",
        "- Need maximum performance\n",
        "- Complex reward signals\n",
        "- Online data collection\n",
        "- Full control over optimization\n",
        "\n",
        "### Next Steps\n",
        "- Scale up with more preference data\n",
        "- Try different β values\n",
        "- Combine with constitutional AI\n",
        "- Implement iterative DPO"
      ]
    }
  ]
}
