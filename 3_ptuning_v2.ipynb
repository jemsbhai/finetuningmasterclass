{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# P-Tuning v2 Example\n",
        "\n",
        "This notebook demonstrates P-Tuning v2, which learns continuous prompts at every transformer layer.\n",
        "\n",
        "**Key Features:**\n",
        "- Learn virtual tokens instead of discrete prompts\n",
        "- Freeze all model weights\n",
        "- Insert trainable prompts at multiple layers\n",
        "- Extremely parameter efficient (~0.1% trainable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    PromptTuningConfig,\n",
        "    TaskType,\n",
        "    PromptTuningInit,\n",
        "    PrefixTuningConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load AG News dataset for text classification\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Use smaller subset\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# Label mappings\n",
        "label_list = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "print(f\"\\nExample:\")\n",
        "print(f\"Text: {train_dataset[0]['text'][:100]}...\")\n",
        "print(f\"Label: {label_list[train_dataset[0]['label']]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    id2label={i: label for i, label in enumerate(label_list)},\n",
        "    label2id={label: i for i, label in enumerate(label_list)}\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure P-Tuning v2 (Prefix Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# P-Tuning v2 is implemented as Prefix Tuning in PEFT\n",
        "ptuning_config = PrefixTuningConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    num_virtual_tokens=20,  # Number of virtual tokens to prepend\n",
        "    encoder_hidden_size=128,  # Hidden size of the encoder\n",
        "    prefix_projection=True,  # Use MLP reparameterization\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Apply P-Tuning to model\n",
        "model = get_peft_model(model, ptuning_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move to device\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_function(examples):\n",
        "    outputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "tokenized_eval = eval_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"Datasets tokenized successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ptuning_results\",\n",
        "    learning_rate=1e-3,  # Higher LR for prompt tuning\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "accuracy = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(f\"Training steps: {len(tokenized_train) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting P-Tuning v2 training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the prompt embeddings\n",
        "model.save_pretrained(\"./ptuning_adapter\")\n",
        "print(\"\\nP-Tuning adapter saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"Evaluation Results:\")\n",
        "print(f\"Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "    \n",
        "    label = model.config.id2label[predicted_class]\n",
        "    confidence = predictions[0][predicted_class].item()\n",
        "    \n",
        "    return label, confidence\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"The stock market reached new highs today as investors showed confidence.\",\n",
        "    \"Scientists discovered a new exoplanet that might support life.\",\n",
        "    \"The championship game ended in overtime with a thrilling victory.\",\n",
        "    \"New smartphone features revolutionary AI capabilities.\",\n",
        "    \"Peace talks continue between the two nations.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    label, confidence = predict(text)\n",
        "    print(f\"Text: {text[:60]}...\")\n",
        "    print(f\"Prediction: {label} (Confidence: {confidence:.2%})\")\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare with Prompt Tuning (Basic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Basic Prompt Tuning (input-only prompts)\n",
        "prompt_tuning_config = PromptTuningConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
        "    num_virtual_tokens=8,  # Fewer tokens at input only\n",
        "    tokenizer_name_or_path=model_name\n",
        ")\n",
        "\n",
        "print(\"Comparison:\")\n",
        "print(\"\\nPrompt Tuning (Basic):\")\n",
        "print(\"- Adds soft prompts only at input\")\n",
        "print(\"- Fewer parameters\")\n",
        "print(\"- Good for simple tasks\")\n",
        "\n",
        "print(\"\\nP-Tuning v2 (Prefix Tuning):\")\n",
        "print(\"- Adds prompts at every layer\")\n",
        "print(\"- More capacity\")\n",
        "print(\"- Better for complex tasks\")\n",
        "print(\"- Matches full fine-tuning on many benchmarks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Multi-Task Learning with P-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# P-Tuning enables efficient multi-task learning\n",
        "print(\"Multi-Task Learning with P-Tuning:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n1. Train separate prompt embeddings for each task\")\n",
        "print(\"2. Keep base model frozen and shared\")\n",
        "print(\"3. Switch tasks by swapping prompt embeddings\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"- Task 1: News Classification (20 virtual tokens)\")\n",
        "print(\"- Task 2: Sentiment Analysis (20 virtual tokens)\")\n",
        "print(\"- Task 3: Topic Classification (20 virtual tokens)\")\n",
        "print(\"\\nMemory:\")\n",
        "print(\"- Base Model: 440MB (shared)\")\n",
        "print(\"- Each Task: ~1MB (prompts only)\")\n",
        "print(\"- Total for 3 tasks: 440MB + 3MB = 443MB\")\n",
        "print(\"- vs Full Fine-tuning: 440MB Ã— 3 = 1,320MB\")\n",
        "\n",
        "# Code structure for multi-task\n",
        "print(\"\\nCode structure:\")\n",
        "print(\"\"\"```python\n",
        "# Load base model once\n",
        "base_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load task-specific prompts\n",
        "news_prompts = load_prompts(\"news_task\")\n",
        "sentiment_prompts = load_prompts(\"sentiment_task\")\n",
        "\n",
        "# Switch tasks dynamically\n",
        "model.set_prompts(news_prompts)  # For news classification\n",
        "model.set_prompts(sentiment_prompts)  # For sentiment analysis\n",
        "```\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### P-Tuning v2 Benefits\n",
        "1. **Ultra-efficient**: Only ~0.1% parameters trainable\n",
        "2. **No model updates**: Base model completely frozen\n",
        "3. **Multi-task ready**: One model, many task prompts\n",
        "4. **Fast switching**: Change tasks by swapping prompts\n",
        "5. **Matches performance**: Often equals full fine-tuning\n",
        "\n",
        "### Best Practices\n",
        "- Use 10-20 virtual tokens for most tasks\n",
        "- Higher learning rates (1e-3) than full fine-tuning\n",
        "- Prefix projection (MLP) improves performance\n",
        "- Works best with models >1B parameters\n",
        "\n",
        "### When to Use P-Tuning\n",
        "- Multiple tasks on same model\n",
        "- Extremely limited memory\n",
        "- Need to preserve base model exactly\n",
        "- Rapid task switching required"
      ]
    }
  ]
}
