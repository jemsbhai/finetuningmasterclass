{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ea9979",
   "metadata": {},
   "source": [
    "# Performance Metrics for LMs & GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U evaluate datasets rouge_score sacrebleu transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd91767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "\n",
    "# Perplexity demo with small causal LM\n",
    "tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "texts = [\"Language models estimate distributions over text.\", \"PEFT reduces trainable parameters.\"]\n",
    "enc = tok(texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    out = model(**enc, labels=enc[\"input_ids\"])\n",
    "ppl = math.exp(out.loss.item())\n",
    "print(\"Perplexity (toy batch):\", ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE/BLEU demo on toy summaries\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "preds = [\"LoRA enables efficient adaptation of LMs.\"]\n",
    "refs  = [[\"LoRA makes adapting language models efficient.\"]]\n",
    "print(\"ROUGE:\", rouge.compute(predictions=preds, references=[r[0] for r in refs]))\n",
    "print(\"BLEU:\", bleu.compute(predictions=preds, references=refs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration (ECE) for a binary classifier (synthetic)\n",
    "import numpy as np\n",
    "\n",
    "# Fake probs and labels\n",
    "probs = np.array([0.05,0.2,0.3,0.4,0.6,0.7,0.85,0.9])\n",
    "labels = np.array([0,0,0,1,1,1,1,1])\n",
    "\n",
    "def ece(probs, labels, bins=5):\n",
    "    bins_idx = np.clip((probs*bins).astype(int), 0, bins-1)\n",
    "    total = len(probs)\n",
    "    acc = 0.0\n",
    "    for b in range(bins):\n",
    "        mask = bins_idx == b\n",
    "        if mask.sum() == 0: \n",
    "            continue\n",
    "        conf = probs[mask].mean()\n",
    "        accu = labels[mask].mean()\n",
    "        acc += (mask.sum()/total) * abs(accu - conf)\n",
    "    return acc\n",
    "\n",
    "print(\"ECE (toy):\", ece(probs, labels, bins=5))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
