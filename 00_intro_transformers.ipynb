{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c272a87",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning & Transformers (Quick Refresher)\n",
    "Minimal setup just to verify your Colab environment and run a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Python:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal deps\n",
    "!pip -q install -U transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tok(\"Transformers are attention-based sequence models.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "print(\"Hidden states shape:\", out.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134f050",
   "metadata": {},
   "source": [
    "> Tip: This workshop focuses on fine-tuning methods; this notebook is intentionally light."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
