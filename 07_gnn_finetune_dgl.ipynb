{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0525aa5f",
   "metadata": {},
   "source": [
    "# Fine‑Tuning for Graph Neural Networks (DGL)\n",
    "Pretrain a GCN on Cora, then fine‑tune (freeze MP, train readout) on Citeseer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U dgl torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, hid, activation=F.relu)\n",
    "        self.conv2 = dgl.nn.GraphConv(hid, out_feats)\n",
    "        self.readout = nn.Identity()  # placeholder\n",
    "    def forward(self, g, x):\n",
    "        h = self.conv1(g, x)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ce8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain on Cora (node classification)\n",
    "cora = CoraGraphDataset()\n",
    "g = cora[0].to(device)\n",
    "feat = g.ndata[\"feat\"].to(device)\n",
    "labels = g.ndata[\"label\"].to(device)\n",
    "train_mask = g.ndata[\"train_mask\"]\n",
    "val_mask = g.ndata[\"val_mask\"]\n",
    "test_mask = g.ndata[\"test_mask\"]\n",
    "\n",
    "model = GCN(feat.shape[1], 64, cora.num_classes).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    logits = model(g, feat)\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = (logits[val_mask].argmax(1) == labels[val_mask]).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | loss {loss.item():.3f} | val acc {val_acc:.3f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"gcn_cora.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine‑tune on Citeseer (freeze convs, train only a new linear readout)\n",
    "cit = CiteseerGraphDataset()\n",
    "g2 = cit[0].to(device)\n",
    "feat2 = g2.ndata[\"feat\"].to(device)\n",
    "labels2 = g2.ndata[\"label\"].to(device)\n",
    "train2 = g2.ndata[\"train_mask\"]\n",
    "val2 = g2.ndata[\"val_mask\"]\n",
    "test2 = g2.ndata[\"test_mask\"]\n",
    "\n",
    "ft_model = GCN(feat2.shape[1], 64, cit.num_classes).to(device)\n",
    "ft_model.load_state_dict(torch.load(\"gcn_cora.pt\"), strict=False)\n",
    "\n",
    "for p in [ft_model.conv1, ft_model.conv2]:\n",
    "    for param in p.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# New readout: map hidden representation to Citeseer classes\n",
    "ft_model.conv2 = dgl.nn.GraphConv(64, cit.num_classes).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([p for p in ft_model.parameters() if p.requires_grad], lr=5e-3)\n",
    "\n",
    "for epoch in range(60):\n",
    "    ft_model.train()\n",
    "    logits = ft_model(g2, feat2)\n",
    "    loss = F.cross_entropy(logits[train2], labels2[train2])\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    if epoch % 10 == 0:\n",
    "        ft_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = (logits[val2].argmax(1) == labels2[val2]).float().mean().item()\n",
    "        print(f\"[FT] Epoch {epoch} | loss {loss.item():.3f} | val acc {val_acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
