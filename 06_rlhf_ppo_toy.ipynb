{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5698a2",
   "metadata": {},
   "source": [
    "# RLHF with PPO (Toy Example)\n",
    "Optimize a small LM against a sentimentâ€‘based reward for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bedac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets accelerate trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8494b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "base = \"distilgpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(base)\n",
    "tok.pad_token = tok.eos_token\n",
    "policy = AutoModelForCausalLM.from_pretrained(base).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reward: sentiment pipeline as a proxy (not an actual RM)\n",
    "sent = pipeline(\"sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ef1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts\n",
    "prompts = [f\"Review: {t}\" for t in [\n",
    "    \"The film was stunningly beautiful.\",\n",
    "    \"The product broke after two days.\",\n",
    "    \"The service was slow but friendly.\",\n",
    "    \"The game mechanics were addictive.\"\n",
    "]] * 32\n",
    "ds = Dataset.from_dict({\"prompt\": prompts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adedea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_cfg = PPOConfig(\n",
    "    model_name=base,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=8,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimize_cuda_cache=True\n",
    ")\n",
    "trainer = PPOTrainer(ppo_cfg, policy, tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO loop (few iterations for demo)\n",
    "gen_kwargs = dict(max_new_tokens=32, do_sample=True, top_p=0.9, temperature=1.0, pad_token_id=tok.eos_token_id)\n",
    "\n",
    "for i in range(3):\n",
    "    batch = ds.shuffle().select(range(ppo_cfg.batch_size))\n",
    "    texts = batch[\"prompt\"]\n",
    "    inputs = tok(texts, return_tensors=\"pt\", padding=True).to(policy.device)\n",
    "    responses = policy.generate(**inputs, **gen_kwargs)\n",
    "    decoded = tok.batch_decode(responses, skip_special_tokens=True)\n",
    "\n",
    "    # compute rewards (positive sentiment => higher reward)\n",
    "    rewards = []\n",
    "    for out in decoded:\n",
    "        res = sent(out)[0]\n",
    "        rewards.append( +1.0 if res[\"label\"]==\"POSITIVE\" else -1.0 )\n",
    "\n",
    "    trainer.step(inputs[\"input_ids\"], responses, torch.tensor(rewards).to(policy.device))\n",
    "    print(f\"Iter {i+1} done. Mean reward: {sum(rewards)/len(rewards):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9201fa2",
   "metadata": {},
   "source": [
    "> Note: This is a didactic example. Real RLHF uses a trained reward model, careful KL control, and stronger baselines."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
