{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb63a48",
   "metadata": {},
   "source": [
    "# PEFT Tour: LoRA vs Prefix vs Prompt (Side‑by‑Side)\n",
    "Compare parameter counts and basic setup across methods on a causal LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41819cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers peft accelerate datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, PrefixTuningConfig, PromptTuningConfig, TaskType\n",
    "\n",
    "base = \"distilgpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(base)\n",
    "tok.pad_token = tok.eos_token\n",
    "model_base = AutoModelForCausalLM.from_pretrained(base)\n",
    "print(\"Base params:\", sum(p.numel() for p in model_base.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA\n",
    "lora_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=[\"c_attn\",\"c_proj\"], lora_dropout=0.05, task_type=TaskType.CAUSAL_LM)\n",
    "lora_model = get_peft_model(model_base, lora_cfg)\n",
    "print(\"LoRA trainable params:\", sum(p.numel() for p in lora_model.parameters() if p.requires_grad)/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix\n",
    "pre_cfg = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
    "pre_model = get_peft_model(AutoModelForCausalLM.from_pretrained(base), pre_cfg)\n",
    "print(\"Prefix trainable params:\", sum(p.numel() for p in pre_model.parameters() if p.requires_grad)/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "pro_cfg = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=20)\n",
    "pro_model = get_peft_model(AutoModelForCausalLM.from_pretrained(base), pro_cfg)\n",
    "print(\"Prompt trainable params:\", sum(p.numel() for p in pro_model.parameters() if p.requires_grad)/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138a98c",
   "metadata": {},
   "source": [
    "**Exercise:** Plug any of these PEFT configs into your SFT trainer and compare convergence vs full‑fine‑tune."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
