{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b1216b",
   "metadata": {},
   "source": [
    "# Inference Optimization: Quantization & Efficient Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413180bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "print(\"Loading 8-bit model…\")\n",
    "model_8 = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "prompt = \"Explain parameter-efficient fine-tuning in one sentence:\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model_8.device)\n",
    "\n",
    "start = time.time()\n",
    "_ = model_8.generate(**inputs, max_new_tokens=64)\n",
    "t8 = time.time()-start\n",
    "print(\"8-bit latency (one pass):\", round(t8,3),\"s\")\n",
    "\n",
    "print(\"Loading 4-bit model…\")\n",
    "model_4 = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model_4.device)\n",
    "\n",
    "start = time.time()\n",
    "_ = model_4.generate(**inputs, max_new_tokens=64)\n",
    "t4 = time.time()-start\n",
    "print(\"4-bit latency (one pass):\", round(t4,3),\"s\")\n",
    "\n",
    "# Optional: torch.compile for supported backends (PyTorch 2.x)\n",
    "try:\n",
    "    model_c = torch.compile(model_4)  # may be a no-op on some environments\n",
    "    start = time.time()\n",
    "    _ = model_c.generate(**inputs, max_new_tokens=64)\n",
    "    tc = time.time()-start\n",
    "    print(\"Compiled latency (one pass):\", round(tc,3),\"s\")\n",
    "except Exception as e:\n",
    "    print(\"torch.compile not available or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c07d2c",
   "metadata": {},
   "source": [
    "> Tips: Use KV-cache, larger batch sizes, and continuous batching for throughput. For serving, consider vLLM/TensorRT-LLM."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
